{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_01_Quiz.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaMS/Natural-Language-Processing-in-TensorFlow/blob/master/Week_01_Quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsTwWPZphHy6",
        "colab_type": "text"
      },
      "source": [
        "# Week 1 Quiz\n",
        "1. What is the name of the object used to tokenize sentences?\n",
        "> Tokenizer\n",
        "\n",
        "\n",
        "2. What is the name of the method used to tokenize a list of sentences?\n",
        "> fit_on_texts(sentences)\n",
        "\n",
        "\n",
        "3. Once you have the corpus tokenized, what’s the method used to encode a list of sentences to use those tokens?\n",
        "> text_to_sequences(sentences)\n",
        "\n",
        "\n",
        "4. When initializing the tokenizer, how to you specify a token to use for unknown words?\n",
        "> oov_token= <Token>\n",
        "\n",
        "  \n",
        "5. If you don’t use a token for out of vocabulary words, what happens at encoding?\n",
        "\n",
        "\n",
        "> The word isn't encoded, and is skipped in the sequence\n",
        "\n",
        "  \n",
        "6. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n",
        "\n",
        "\n",
        "> Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace\n",
        "  \n",
        "\n",
        "7. If you have a number of sequences of different length, and call pad_sequences on them, what’s the default result?\n",
        "\n",
        "\n",
        "> They'll get padded to the length of the longest sequence by adding zeros at the beginning of shorter ones\n",
        "\n",
        "\n",
        "8. When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?\n",
        "\n",
        "\n",
        "> Pass padding='post' to pad_sequences when initializing it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}